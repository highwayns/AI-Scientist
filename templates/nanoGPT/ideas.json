[
    {
        "Name": "adaptive_block_size",
        "Title": "Adaptive Block Size: Dynamic Context Window Adjustment for Efficient Training",
        "Experiment": "Modify the model to dynamically adjust its block size during training, starting with a smaller block size and gradually increasing it. This could potentially lead to faster initial training and better long-range dependency learning.",
        "Interestingness": 6,
        "Feasibility": 4,
        "Novelty": 4,
        "novel": true
    },
    {
        "Name": "layerwise_learning_rates",
        "Title": "Layer-wise Learning Rate Adaptation: Optimizing Training Dynamics in Transformer Models",
        "Experiment": "Implement layer-wise learning rates, where each transformer layer has its own learning rate. Modify the configure_optimizers function to assign different learning rates to different layers, with deeper layers having lower learning rates. Compare the training dynamics, convergence speed, and final performance with the baseline model.",
        "Interestingness": 4,
        "Feasibility": 6,
        "Novelty": 2,
        "novel": false
    },
    {
        "Name": "curriculum_learning",
        "Title": "Curriculum Learning for Character-Level Language Models: Adapting Training Sample Complexity",
        "Experiment": "Implement a curriculum learning strategy by modifying the get_batch function to initially sample batches with simpler character sequences (e.g., higher frequency characters or repetitive patterns) and gradually introduce more complex sequences as training progresses. Track the convergence speed and final performance compared to the baseline model without curriculum learning.",
        "Interestingness": 7,
        "Feasibility": 5,
        "Novelty": 6,
        "novel": true
    },
    {
        "Name": "adaptive_attention",
        "Title": "Adaptive Attention Mechanism: Dynamically Scaling Focus in Transformer Models",
        "Experiment": "Enhance the CausalSelfAttention class by normalizing the attention scores using a dynamically computed factor based on input sequence properties, such as variance or entropy of token embeddings, before applying the softmax function. This avoids introducing new parameters while still allowing adaptive focus. Evaluate the model's performance in convergence speed and accuracy against the baseline model.",
        "Interestingness": 8,
        "Feasibility": 6,
        "Novelty": 7,
        "novel": true
    }
]