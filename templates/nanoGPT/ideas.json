[
    {
        "Name": "adaptive_block_size",
        "Title": "Adaptive Block Size: Dynamic Context Window Adjustment for Efficient Training",
        "Experiment": "Modify the model to dynamically adjust its block size during training, starting with a smaller block size and gradually increasing it. This could potentially lead to faster initial training and better long-range dependency learning.",
        "Interestingness": 6,
        "Feasibility": 4,
        "Novelty": 4,
        "novel": true
    },
    {
        "Name": "layerwise_learning_rates",
        "Title": "Layer-wise Learning Rate Adaptation: Optimizing Training Dynamics in Transformer Models",
        "Experiment": "Implement layer-wise learning rates, where each transformer layer has its own learning rate. Modify the configure_optimizers function to assign different learning rates to different layers, with deeper layers having lower learning rates. Compare the training dynamics, convergence speed, and final performance with the baseline model.",
        "Interestingness": 4,
        "Feasibility": 6,
        "Novelty": 2,
        "novel": true
    },
    {
        "Name": "curriculum_learning",
        "Title": "Curriculum Learning: Enhancing Language Model Training through Gradual Complexity",
        "Experiment": "Implement curriculum learning by modifying the get_batch function to initially sample simpler sequences, defined by shorter lengths or lower entropy in character distribution. Gradually increase the complexity of sequences by lengthening them or increasing character diversity as training progresses. Evaluate the impact on convergence speed, final performance, and generalization by comparing against a baseline model trained without curriculum learning.",
        "Interestingness": 8,
        "Feasibility": 6,
        "Novelty": 7,
        "novel": false
    },
    {
        "Name": "multi_task_learning",
        "Title": "Integrating Multi-Task Learning: Enhancing Character-Level Language Models",
        "Experiment": "Introduce a multi-task learning framework by modifying the training loop to include related tasks like next character prediction or character classification. Adjust the model to output logits for each task and implement a combined loss function to jointly optimize them. Evaluate the impact on the model's performance and convergence compared to a baseline single-task model.",
        "Interestingness": 7,
        "Feasibility": 5,
        "Novelty": 6,
        "novel": true
    }
]